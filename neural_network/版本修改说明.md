## 5.8
经过一番折腾，找打了一些有趣的问题。
sklearn关于NN的[官方文档](http://sklearn.apachecn.org/cn/0.19.0/auto_examples/neural_networks/plot_mlp_alpha.html#sphx-glr-auto-examples-neural-networks-plot-mlp-alpha-py)给了以下的案例：
![](http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_mlp_alpha_001.png)
值得注意的是网上的教程基本已sigmoid函数作为激活函数，在使用该函数的时候有两个需要注意的地方：
1. 最终的决策边界非常容易呈线性（暂时也不知道为什么）；
2. sigmoid因为误差逆传播时梯度消失的原因几乎解决不了局部最小的问题；

关于上面的第一点可以自行去[官方文档](http://sklearn.apachecn.org/cn/0.19.0/auto_examples/neural_networks/plot_mlp_alpha.html#sphx-glr-auto-examples-neural-networks-plot-mlp-alpha-py)那儿拷代码然后把激活函数换成logistic后验证
至于第二点可以用本库的代码验证，采用了文档第二个make_circle的例子，损失函数采用平方差和，100个样本最终会稳定在25的损失值（原因是平面上的任何点最后的预测结果都是0.5,0.5）

关于NN的实践暂时到这里为止，有空会把激活函数换成relu再写一遍。
